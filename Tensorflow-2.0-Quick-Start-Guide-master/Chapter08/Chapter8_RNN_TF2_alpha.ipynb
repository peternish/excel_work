{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2018 The TensorFlow Authors.\n",
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#https://github.com/tensorflow/docs/blob/master/site/en/tutorials/sequences/text_generation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file='1400-0.txt'\n",
    "url='https://www.gutenberg.org/files/1400/1400-0.txt' # Great Expectations by Charles Dickens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = tf.keras.utils.get_file(file,url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1013445 characters\n"
     ]
    }
   ],
   "source": [
    "text = open(path).read()\n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My father's family name being Pirrip, and my Christian name Philip, my\n",
      "infant tongue could make of both names nothing longer or more explicit\n",
      "than Pip. So, I called myself Pip, and came to be called Pip.\n",
      "\n",
      "I give Pirrip as my father's family name, on the authority of his\n",
      "tombstone and my sister,--Mrs\n"
     ]
    }
   ],
   "source": [
    "# strip off text we don't need\n",
    "text = text[835:]\n",
    "\n",
    "# Take a look at the first 300 characters in text\n",
    "print(text[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84 unique characters.\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocabulary = sorted(set(text))\n",
    "print ('{} unique characters.'.format(len(vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '%': 4, '&': 5, \"'\": 6, '(': 7, ')': 8, '*': 9, ',': 10, '-': 11, '.': 12, '/': 13, '0': 14, '1': 15, '2': 16, '3': 17, '4': 18, '5': 19, '6': 20, '7': 21, '8': 22, '9': 23, ':': 24, ';': 25, '?': 26, '@': 27, 'A': 28, 'B': 29, 'C': 30, 'D': 31, 'E': 32, 'F': 33, 'G': 34, 'H': 35, 'I': 36, 'J': 37, 'K': 38, 'L': 39, 'M': 40, 'N': 41, 'O': 42, 'P': 43, 'Q': 44, 'R': 45, 'S': 46, 'T': 47, 'U': 48, 'V': 49, 'W': 50, 'X': 51, 'Y': 52, 'Z': 53, 'a': 54, 'b': 55, 'c': 56, 'd': 57, 'e': 58, 'f': 59, 'g': 60, 'h': 61, 'i': 62, 'j': 63, 'k': 64, 'l': 65, 'm': 66, 'n': 67, 'o': 68, 'p': 69, 'q': 70, 'r': 71, 's': 72, 't': 73, 'u': 74, 'v': 75, 'w': 76, 'x': 77, 'y': 78, 'z': 79, 'ê': 80, 'ô': 81, '“': 82, '”': 83}\n"
     ]
    }
   ],
   "source": [
    "# Creating a dictionary of unique characters to indices\n",
    "char_to_index = {char:index for index, char in enumerate(vocabulary)}\n",
    "print(char_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n' ' ' '!' '$' '%' '&' \"'\" '(' ')' '*' ',' '-' '.' '/' '0' '1' '2' '3'\n",
      " '4' '5' '6' '7' '8' '9' ':' ';' '?' '@' 'A' 'B' 'C' 'D' 'E' 'F' 'G' 'H'\n",
      " 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W' 'X' 'Y' 'Z'\n",
      " 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o' 'p' 'q' 'r'\n",
      " 's' 't' 'u' 'v' 'w' 'x' 'y' 'z' 'ê' 'ô' '“' '”']\n"
     ]
    }
   ],
   "source": [
    "index_to_char = np.array(vocabulary)\n",
    "print(index_to_char)\n",
    "text_as_int = np.array([char_to_index[char] for char in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '\\n':   0,\n",
      "  ' ' :   1,\n",
      "  '!' :   2,\n",
      "  '$' :   3,\n",
      "  '%' :   4,\n",
      "  '&' :   5,\n",
      "  \"'\" :   6,\n",
      "  '(' :   7,\n",
      "  ')' :   8,\n",
      "  '*' :   9,\n",
      "  ',' :  10,\n",
      "  '-' :  11,\n",
      "  '.' :  12,\n",
      "  '/' :  13,\n",
      "  '0' :  14,\n",
      "  '1' :  15,\n",
      "  '2' :  16,\n",
      "  '3' :  17,\n",
      "  '4' :  18,\n",
      "  '5' :  19,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print('{')\n",
    "for char,_ in zip(char_to_index, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char_to_index[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"My father's fam\" ---- characters mapped to int ---- > [40 78  1 59 54 73 61 58 71  6 72  1 59 54 66]\n"
     ]
    }
   ],
   "source": [
    "# Show how the first 15 characters from the text are mapped to integers\n",
    "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:15]), text_as_int[:15]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M\n",
      "y\n",
      " \n",
      "f\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "sequence_length = 100\n",
    "examples_per_epoch = len(text)//sequence_length\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for char in char_dataset.take(5):\n",
    "  print(index_to_char[char.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"My father's family name being Pirrip, and my Christian name Philip, my\\ninfant tongue could make of bo\"\n",
      "'th names nothing longer or more explicit\\nthan Pip. So, I called myself Pip, and came to be called Pip'\n",
      "\".\\n\\nI give Pirrip as my father's family name, on the authority of his\\ntombstone and my sister,--Mrs. J\"\n",
      "'oe Gargery, who married the blacksmith.\\nAs I never saw my father or my mother, and never saw any like'\n",
      "'ness\\nof either of them (for their days were long before the days of\\nphotographs), my first fancies re'\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(sequence_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "  print(repr(''.join(index_to_char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  \"My father's family name being Pirrip, and my Christian name Philip, my\\ninfant tongue could make of b\"\n",
      "Target data: \"y father's family name being Pirrip, and my Christian name Philip, my\\ninfant tongue could make of bo\"\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "  print ('Input data: ', repr(''.join(index_to_char[input_example.numpy()])))\n",
    "  print ('Target data:', repr(''.join(index_to_char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 40 ('M')\n",
      "  expected output: 78 ('y')\n",
      "Step    1\n",
      "  input: 78 ('y')\n",
      "  expected output: 1 (' ')\n",
      "Step    2\n",
      "  input: 1 (' ')\n",
      "  expected output: 59 ('f')\n",
      "Step    3\n",
      "  input: 59 ('f')\n",
      "  expected output: 54 ('a')\n",
      "Step    4\n",
      "  input: 54 ('a')\n",
      "  expected output: 73 ('t')\n"
     ]
    }
   ],
   "source": [
    "for char, (input_index, target_index) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(char))\n",
    "    print(\"  input: {} ({:s})\".format(input_index, repr(index_to_char[input_index])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_index, repr(index_to_char[target_index])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RepeatDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Batch size\n",
    "batch = 64\n",
    "steps_per_epoch = examples_per_epoch//batch\n",
    "\n",
    "# TF data maintains a buffer in memory in which to shuffle data\n",
    "# since it is designed to work with possibly endless data\n",
    "buffer = 10000\n",
    "\n",
    "dataset = dataset.shuffle(buffer).batch(batch, drop_remainder=True)\n",
    "\n",
    "dataset = dataset.repeat()\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  The vocabulary length in characterrs\n",
    "vocabulary_length = len(vocabulary)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dimension = 256\n",
    "\n",
    "# Number of RNN units\n",
    "recurrent_nn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU in use\n"
     ]
    }
   ],
   "source": [
    "if tf.test.is_gpu_available():\n",
    "    recurrent_nn = tf.compat.v1.keras.layers.CuDNNGRU\n",
    "    print(\"GPU in use\")\n",
    "else:\n",
    "    import functools\n",
    "    recurrent_nn = functools.partial(tf.keras.layers.GRU, recurrent_activation='sigmoid')\n",
    "    print(\"CPU in use\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model(vocabulary_size, embedding_dimension, recurrent_nn_units, batch_size):\n",
    "    model = tf.keras.Sequential(\n",
    "        [tf.keras.layers.Embedding(vocabulary_size, embedding_dimension, batch_input_shape=[batch_size, None]),\n",
    "    recurrent_nn(recurrent_nn_units, return_sequences=True, recurrent_initializer='glorot_uniform', stateful=True),\n",
    "    tf.keras.layers.Dense(vocabulary_length)\n",
    "  ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocabulary_size = len(vocabulary),\n",
    "  embedding_dimension=embedding_dimension,\n",
    "  recurrent_nn_units=recurrent_nn_units,\n",
    "  batch_size=batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 84) # (batch, sequence_length, vocabulary_length)\n"
     ]
    }
   ],
   "source": [
    "for batch_input_example, batch_target_example in dataset.take(1):\n",
    "    batch_predictions_example = model(batch_input_example)\n",
    "    print(batch_predictions_example.shape, \"# (batch, sequence_length, vocabulary_length)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           21504     \n",
      "_________________________________________________________________\n",
      "unified_gru (UnifiedGRU)     (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 84)            86100     \n",
      "=================================================================\n",
      "Total params: 4,045,908\n",
      "Trainable params: 4,045,908\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(logits=batch_predictions_example[0], num_samples=1)\n",
    "\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([61, 33, 47, 79, 37, 58, 25, 71, 28, 81, 24, 34,  9,  6, 83, 77, 18,\n",
       "       57, 26,  5, 81, 56, 58, 23, 44, 58, 64, 39, 24,  9, 42, 21, 27, 38,\n",
       "       74, 68, 53, 40,  5, 82,  3, 71, 14, 66, 60,  0,  4, 13, 16, 11, 20,\n",
       "       44, 54, 32,  5,  3,  8, 13,  6, 52, 22, 66, 12, 77,  8, 23, 18, 55,\n",
       "       26, 59, 38, 69, 12, 71, 45, 81, 12, 17, 36, 40, 40, 47, 40, 63, 40,\n",
       "       40, 54, 60, 12, 62, 39, 15, 39, 74, 40, 20,  1, 26,  6, 25])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " 'r, that I might refer to it again; but I could not find it, and\\nwas uneasy to think that it must hav'\n",
      "Next Char Predictions: \n",
      " \"hFTzJe;rAô:G*'”x4d?&ôce9QekL:*O7@KuoZM&“$r0mg\\n%/2-6QaE&$)/'Y8m.x)94b?fKp.rRô.3IMMTMjMMag.iL1LuM6 ?';\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(index_to_char[batch_input_example[0]])))\n",
    "\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(index_to_char[sampled_indices ])))\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 84)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.4306927\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_loss_example  = tf.compat.v1.losses.sparse_softmax_cross_entropy(batch_target_example, batch_predictions_example)\n",
    "print(\"Prediction shape: \", batch_predictions_example.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", batch_loss_example.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next produced by upgrade script.... \n",
    "#model.compile(optimizer = tf.compat.v1.train.AdamOptimizer(), loss = loss) \n",
    "#.... but following optimizer is available.\n",
    "model.compile(optimizer = tf.optimizers.Adam(), loss = loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "directory = './checkpoints'\n",
    "# Name of the checkpoint files\n",
    "file_prefix = os.path.join(directory, \"ckpt_{epoch}\")\n",
    "\n",
    "callback=[tf.keras.callbacks.ModelCheckpoint(filepath=file_prefix, save_weights_only=True)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/45\n",
      "158/158 [==============================] - 107s 675ms/step - loss: 2.6684\n",
      "Epoch 2/45\n",
      "158/158 [==============================] - 104s 656ms/step - loss: 1.9597\n",
      "Epoch 3/45\n",
      "158/158 [==============================] - 103s 654ms/step - loss: 1.6832\n",
      "Epoch 4/45\n",
      "158/158 [==============================] - 104s 657ms/step - loss: 1.5192\n",
      "Epoch 5/45\n",
      "158/158 [==============================] - 105s 664ms/step - loss: 1.4198\n",
      "Epoch 6/45\n",
      "158/158 [==============================] - 104s 659ms/step - loss: 1.3533\n",
      "Epoch 7/45\n",
      "158/158 [==============================] - 104s 657ms/step - loss: 1.3040\n",
      "Epoch 8/45\n",
      "158/158 [==============================] - 105s 662ms/step - loss: 1.2615\n",
      "Epoch 9/45\n",
      "158/158 [==============================] - 104s 657ms/step - loss: 1.2278\n",
      "Epoch 10/45\n",
      "158/158 [==============================] - 104s 659ms/step - loss: 1.1944\n",
      "Epoch 11/45\n",
      "158/158 [==============================] - 103s 654ms/step - loss: 1.1610\n",
      "Epoch 12/45\n",
      "158/158 [==============================] - 105s 663ms/step - loss: 1.1296\n",
      "Epoch 13/45\n",
      "158/158 [==============================] - 104s 661ms/step - loss: 1.0959\n",
      "Epoch 14/45\n",
      "158/158 [==============================] - 106s 670ms/step - loss: 1.0633\n",
      "Epoch 15/45\n",
      "158/158 [==============================] - 106s 671ms/step - loss: 1.0313\n",
      "Epoch 16/45\n",
      "158/158 [==============================] - 104s 661ms/step - loss: 0.9981\n",
      "Epoch 17/45\n",
      "158/158 [==============================] - 104s 659ms/step - loss: 0.9632\n",
      "Epoch 18/45\n",
      "158/158 [==============================] - 106s 669ms/step - loss: 0.9298\n",
      "Epoch 19/45\n",
      "158/158 [==============================] - 104s 658ms/step - loss: 0.8970\n",
      "Epoch 20/45\n",
      "158/158 [==============================] - 104s 656ms/step - loss: 0.8669\n",
      "Epoch 21/45\n",
      "158/158 [==============================] - 103s 655ms/step - loss: 0.8343\n",
      "Epoch 22/45\n",
      "158/158 [==============================] - 104s 658ms/step - loss: 0.8059\n",
      "Epoch 23/45\n",
      "158/158 [==============================] - 104s 660ms/step - loss: 0.7816\n",
      "Epoch 24/45\n",
      "158/158 [==============================] - 105s 666ms/step - loss: 0.7559\n",
      "Epoch 25/45\n",
      "158/158 [==============================] - 104s 661ms/step - loss: 0.7338\n",
      "Epoch 26/45\n",
      "158/158 [==============================] - 105s 664ms/step - loss: 0.7152\n",
      "Epoch 27/45\n",
      "158/158 [==============================] - 104s 661ms/step - loss: 0.6974\n",
      "Epoch 28/45\n",
      "158/158 [==============================] - 105s 667ms/step - loss: 0.6843\n",
      "Epoch 29/45\n",
      "158/158 [==============================] - 106s 671ms/step - loss: 0.6723\n",
      "Epoch 30/45\n",
      "158/158 [==============================] - 105s 665ms/step - loss: 0.6593\n",
      "Epoch 31/45\n",
      "158/158 [==============================] - 105s 664ms/step - loss: 0.6503\n",
      "Epoch 32/45\n",
      "158/158 [==============================] - 106s 672ms/step - loss: 0.6428\n",
      "Epoch 33/45\n",
      "158/158 [==============================] - 105s 666ms/step - loss: 0.6331\n",
      "Epoch 34/45\n",
      "158/158 [==============================] - 107s 676ms/step - loss: 0.6262\n",
      "Epoch 35/45\n",
      "158/158 [==============================] - 105s 662ms/step - loss: 0.6221\n",
      "Epoch 36/45\n",
      "158/158 [==============================] - 108s 681ms/step - loss: 0.6165\n",
      "Epoch 37/45\n",
      "158/158 [==============================] - 107s 676ms/step - loss: 0.6127\n",
      "Epoch 38/45\n",
      "158/158 [==============================] - 107s 676ms/step - loss: 0.6109\n",
      "Epoch 39/45\n",
      "158/158 [==============================] - 107s 677ms/step - loss: 0.6089\n",
      "Epoch 40/45\n",
      "158/158 [==============================] - 106s 672ms/step - loss: 0.6064\n",
      "Epoch 41/45\n",
      "158/158 [==============================] - 107s 674ms/step - loss: 0.6037\n",
      "Epoch 42/45\n",
      "158/158 [==============================] - 105s 663ms/step - loss: 0.6043\n",
      "Epoch 43/45\n",
      "158/158 [==============================] - 104s 659ms/step - loss: 0.6028\n",
      "Epoch 44/45\n",
      "158/158 [==============================] - 106s 669ms/step - loss: 0.6050\n",
      "Epoch 45/45\n",
      "158/158 [==============================] - 107s 679ms/step - loss: 0.6054\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history = model.fit(dataset, epochs=epochs, steps_per_epoch=steps_per_epoch, callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./checkpoints/ckpt_45'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocabulary_size, embedding_dimension, recurrent_nn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(directory))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            21504     \n",
      "_________________________________________________________________\n",
      "unified_gru_1 (UnifiedGRU)   (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 84)             86100     \n",
      "=================================================================\n",
      "Total params: 4,045,908\n",
      "Trainable params: 4,045,908\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_text(model, start_string, temperature, characters_to_generate):\n",
    "\n",
    "  # Vectorise  start string into numbers\n",
    "    input_string = [char_to_index[char] for char in start_string]\n",
    "    input_string = tf.expand_dims(input_string, 0)\n",
    "\n",
    "  # Empty string to store  generated text\n",
    "    generated = []\n",
    "\n",
    "  # (Batch size is 1)\n",
    "    model.reset_states()\n",
    "    for i in range(characters_to_generate):\n",
    "        predictions = model(input_string)\n",
    "      # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a multinomial distribution to predict the word returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(logits=predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # Pass the predicted word as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "        input_string = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        generated.append(index_to_char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(generated)) # generated is a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pip!”\n",
      "\n",
      "“So it was.”\n",
      "\n",
      "“Astonishing!” said Joe, in the nature of an umbrella.\n",
      "\n",
      "“Then, as it were now as for the other convict. “The fear of our own little nor\n",
      "stones of the forge. I was falling into my head to look if the way by which I had come to be a man. A deserting your mind to him, my indertrodic work in any convicts!” Then both very well. I\n",
      "thought it would have been more crack with the forge and Mill Pond Bank, Clara\n",
      "was not a variety of being in common black piece of paper, and put the two convicts were\n",
      "bought, the more certain I am of a circle.\n",
      "\n",
      "“Lookee here, Pip, look at his door.\n",
      "\n",
      "In the evening there was a lady by which I was always creeping the fire at the windows of the copyright holder), the waiter reappeared.\n",
      "\n",
      "“Why you see, old chap. They'll do you suppose the attempt to do it\n",
      "distinctly thanked him and said that Mr. Pumblechook was on my shoulder by some one\n",
      "of these days, and as Mr. Pumblechook was not at all likely he could\n",
      "have done it. I had done it, under the silence \n"
     ]
    }
   ],
   "source": [
    "# In the arguments, a low temperature gives more predictable text whereas a high temperature gives more random text.\n",
    "# Also this is where you can change the start string.\n",
    "generated_text = generate_text(model=model, start_string=\"Pip\", temperature=0.1, characters_to_generate = 1000)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
