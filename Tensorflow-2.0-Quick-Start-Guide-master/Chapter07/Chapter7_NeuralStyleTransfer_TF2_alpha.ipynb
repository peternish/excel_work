{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "import functools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "# set things up for images display\n",
    "mpl.rcParams['figure.figsize'] = (10,10)\n",
    "mpl.rcParams['axes.grid'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing import image as kp_image\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"TensorFlow version: {}\".format(tf.__version__))\n",
    "print(\"Eager execution is: {}\".format(tf.executing_eagerly()))\n",
    "ran = tf.Variable(42)\n",
    "print(\"Is there a GPU available?: \"),\n",
    "print(tf.test.is_gpu_available())\n",
    "\n",
    "print(\"Is the Tensor on GPU #0?:  \"),\n",
    "print(ran.device.endswith('GPU:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the images we will work with inititially\n",
    "content_path = './tmp/nst/elephant.jpg'#Andrew Shiva / Wikipedia / CC BY-SA 4.0\n",
    "style_path = './tmp/nst/zebra.jpg' # zebra:Yathin S Krishnappa, https://creativecommons.org/licenses/by-sa/4.0/deed.en\n",
    "#Also available\n",
    "#content_path = './tmp/nst/skyscrapers.jpg'#Andrew Shiva / Wikipedia / CC BY-SA 4.0\n",
    "#style_path = './tmp/nst/sunset.jpg'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path_to_image):\n",
    "    max_dimension = 512\n",
    "    image = Image.open(path_to_image)\n",
    "    longest_side = max(image.size)\n",
    "    scale = max_dimension/longest_side\n",
    "    image = image.resize((round(image.size[0]*scale), round(image.size[1]*scale)), Image.ANTIALIAS)\n",
    "\n",
    "    image = kp_image.img_to_array(image) # keras preprocessing\n",
    "\n",
    "    # Broadcast the image array so that it has a batch dimension on axis 0\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(image, title=None):\n",
    "  # Remove the batch dimension\n",
    "    image1 = np.squeeze(image, axis=0)\n",
    "  # Normalize for display\n",
    "    image1 = image1.astype('uint8')\n",
    "    plt.imshow(image1)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.imshow(image1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "channel_means = [103.939, 116.779, 123.68]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "content_image = load_image(content_path).astype('uint8')\n",
    "style_image = load_image(style_path).astype('uint8')\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "show_image(content_image, 'Content Image')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "show_image(style_image, 'Style Image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_image(path_to_image):\n",
    "  image = load_image(path_to_image)\n",
    "  image = tf.keras.applications.vgg19.preprocess_input(image)\n",
    "  return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deprocess_image(processed_image):\n",
    "  im = processed_image.copy()\n",
    "  if len(im.shape) == 4:\n",
    "    im = np.squeeze(im, 0)\n",
    "  assert len(im.shape) == 3, (\"Input to deprocess image must be an image of \"\n",
    "                             \"dimension [1, height, width, channel] or [height, width, channel]\")\n",
    "  if len(im.shape) != 3:\n",
    "    raise ValueError(\"Invalid input to deprocessing image\")\n",
    "\n",
    "  # the inverse of the preprocessiing step\n",
    "  im[:, :, 0] += channel_means[0] # these are the means subracted by the preprocessing step\n",
    "  im[:, :, 1] += channel_means[1]\n",
    "  im[:, :, 2] += channel_means[2]\n",
    "  im= im[:, :, ::-1] # channel last\n",
    "\n",
    "  im = np.clip(im, 0, 255).astype('uint8')\n",
    "  return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The feature maps are obtained from this content layer\n",
    "content_layers = ['block5_conv2']\n",
    "\n",
    "# Style layers we need\n",
    "style_layers = ['block1_conv1',\n",
    "                'block2_conv1',\n",
    "                'block3_conv1',\n",
    "                'block4_conv1',\n",
    "                'block5_conv1'\n",
    "               ]\n",
    "\n",
    "number_of_content_layers = len(content_layers)\n",
    "number_of_style_layers = len(style_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "  # Load VGG model, pretrained on imagenet data\n",
    "  vgg_model = tf.keras.applications.vgg19.VGG19(include_top=False, weights='imagenet')\n",
    "  vgg_model.trainable = False\n",
    "\n",
    "  # Get output layers corresponding to style and content layers\n",
    "  style_outputs = [vgg_model.get_layer(name).output for name in style_layers]\n",
    "  content_outputs = [vgg_model.get_layer(name).output for name in content_layers]\n",
    "  \n",
    "  model_outputs = style_outputs + content_outputs\n",
    "  # Build model\n",
    "  return models.Model(vgg_model.input, model_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rms_loss(image1,image2):\n",
    "    loss = tf.reduce_mean(input_tensor=tf.square(image1 - image2))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_loss(content, target):\n",
    "  return rms_loss(content, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(input_tensor):\n",
    "  channels = int(input_tensor.shape[-1]) # channels is last dimension\n",
    "  tensor = tf.reshape(input_tensor, [-1, channels])  # Make the image channels first\n",
    "  number_of_channels = tf.shape(input=tensor)[0]\n",
    "  gram = tf.matmul(tensor, tensor, transpose_a=True)\n",
    "  return gram / tf.cast(number_of_channels, tf.float32)\n",
    "\n",
    "def style_loss(style, gram_target):\n",
    "  gram_style = gram_matrix(style)\n",
    "  return rms_loss(gram_style, gram_target) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_representations(model, content_path, style_path):\n",
    "  #Function to compute  content and style feature representations.\n",
    "\n",
    "  content_image = load_and_process_image(content_path)\n",
    "  content_outputs = model(content_image)\n",
    "  #content_features = [content_layer[0] for content_layer in content_outputs[:number_of_content_layers]]\n",
    "  content_features = [content_layer[0] for content_layer in content_outputs[number_of_style_layers:]]\n",
    "\n",
    "\n",
    "  style_image = load_and_process_image(style_path)\n",
    "  style_outputs = model(style_image)\n",
    "  style_features = [style_layer[0] for style_layer in style_outputs[:number_of_style_layers]]\n",
    "\n",
    "  return style_features, content_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_total_loss(model, loss_weights, init_image, gram_style_features, content_features):\n",
    "\n",
    "  style_weight, content_weight = loss_weights\n",
    "\n",
    "  model_outputs = model(init_image)\n",
    "\n",
    "\n",
    "\n",
    "  content_score = 0\n",
    "  content_output_features = model_outputs[number_of_style_layers:]\n",
    "  weight_per_content_layer = 1.0 / float(number_of_content_layers)\n",
    "  for target_content, comb_content in zip(content_features, content_output_features):\n",
    "    content_score += weight_per_content_layer*content_loss(comb_content[0], target_content)\n",
    "  content_score *= content_weight\n",
    "\n",
    "\n",
    "  style_score = 0\n",
    "  style_output_features = model_outputs[:number_of_style_layers]\n",
    "  weight_per_style_layer = 1.0 / float(number_of_style_layers)\n",
    "  for target_style, comb_style in zip(gram_style_features, style_output_features):\n",
    "    style_score += weight_per_style_layer *style_loss(comb_style[0], target_style)\n",
    "  style_score *= style_weight\n",
    "\n",
    "\n",
    "  total_loss = style_score + content_score\n",
    "  return total_loss, style_score, content_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grads(config):\n",
    "  with tf.GradientTape() as tape:\n",
    "    all_loss = compute_total_loss(**config)\n",
    "  # Compute gradients wrt input image\n",
    "  total_loss = all_loss[0]\n",
    "  return tape.gradient(total_loss, config['init_image']), all_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display\n",
    "\n",
    "def run_style_transfer(content_path,\n",
    "                       style_path,\n",
    "                       number_of_iterations=1000,\n",
    "                       content_weight=1e3,\n",
    "                       style_weight=1e-2):\n",
    "  # We don't need to (or want to) train any layers of our model, so we set their\n",
    "  # trainable to false.\n",
    "  model = get_model()\n",
    "  for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "  # Get the style and content feature representations (from our specified intermediate layers)\n",
    "  style_features, content_features = get_feature_representations(model, content_path, style_path)\n",
    "  gram_style_features = [gram_matrix(style_feature) for style_feature in style_features]\n",
    "\n",
    "  # Set initial image\n",
    "  initial_image = load_and_process_image(content_path)\n",
    "  initial_image = tf.Variable(initial_image, dtype=tf.float32)\n",
    "  # Create our optimizer\n",
    "  optimiser = tf.compat.v1.train.AdamOptimizer(learning_rate=5, beta1=0.99, epsilon=1e-1)\n",
    "  #opt = tf.keras.optimizers.Adam()\n",
    "\n",
    "  # Store our best result\n",
    "  best_loss, best_image = float('inf'), None # any loss will be lesss than float('inf')\n",
    "\n",
    "  # Create a suitable configuration\n",
    "  loss_weights = (style_weight, content_weight)\n",
    "  config = {\n",
    "      'model': model,\n",
    "      'loss_weights': loss_weights,\n",
    "      'init_image': initial_image,\n",
    "      'gram_style_features': gram_style_features,\n",
    "      'content_features': content_features\n",
    "  }\n",
    "\n",
    "  # For displaying\n",
    "  number_rows = 2\n",
    "  number_cols = 5\n",
    "  display_interval = number_of_iterations/(number_rows*number_cols)\n",
    "\n",
    "  norm_means = np.array(channel_means)\n",
    "  minimum_vals = -norm_means\n",
    "  maximum_vals = 255 - norm_means\n",
    "  images = []\n",
    "  for i in range(number_of_iterations):\n",
    "    grads, all_loss = compute_grads(config)\n",
    "    loss, style_score, content_score = all_loss\n",
    "    optimiser.apply_gradients([(grads, initial_image)])\n",
    "    clipped_image = tf.clip_by_value(initial_image, minimum_vals, maximum_vals)\n",
    "    initial_image.assign(clipped_image)\n",
    "\n",
    "    if loss < best_loss:\n",
    "      # Update best loss and best image from total loss.\n",
    "      best_loss = loss\n",
    "      best_image = deprocess_image(initial_image.numpy()) # this is one place where we need eager execution\n",
    "\n",
    "    if i % display_interval== 0:\n",
    "\n",
    "      # Use the .numpy() method to get the concrete numpy array, needs eager execution\n",
    "      plot_image = initial_image.numpy()\n",
    "      plot_image = deprocess_image(plot_image)\n",
    "      images.append(plot_image)\n",
    "      IPython.display.clear_output(wait=True)\n",
    "      IPython.display.display_png(Image.fromarray(plot_image))\n",
    "      print('Iteration: {}'.format(i))\n",
    "      print('Total loss: {:.4e}, '\n",
    "            'style loss: {:.4e}, '\n",
    "            'content loss: {:.4e} '\n",
    "           .format(loss, style_score, content_score))\n",
    "\n",
    "  IPython.display.clear_output(wait=True)\n",
    "  plt.figure(figsize=(14,4))\n",
    "  for i,image in enumerate(images):\n",
    "      plt.subplot(number_rows,number_cols,i+1)\n",
    "      plt.imshow(image)\n",
    "      plt.xticks([])\n",
    "      plt.yticks([])\n",
    "\n",
    "  return best_image, best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_image, best_loss = run_style_transfer(content_path, style_path, number_of_iterations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Image.fromarray(best_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(best_image, content_path, style_path, show_large_final=True):\n",
    "  plt.figure(figsize=(10, 5))\n",
    "  content = load_image(content_path)\n",
    "  style = load_image(style_path)\n",
    "\n",
    "  plt.subplot(1, 2, 1)\n",
    "  show_image(content, 'Content Image')\n",
    "\n",
    "  plt.subplot(1, 2, 2)\n",
    "  show_image(style, 'Style Image')\n",
    "\n",
    "  if show_large_final:\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    plt.imshow(best_image)\n",
    "    plt.title('Output Image')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_results(best_image, content_path, style_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
