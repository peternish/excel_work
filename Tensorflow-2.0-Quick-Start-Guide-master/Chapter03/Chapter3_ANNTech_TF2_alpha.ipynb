{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data.Dataset Examples with numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy array example\n",
    "num_items = 11\n",
    "num_list1 = np.arange(num_items)\n",
    "num_list2 = np.arange(num_items,num_items*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_list1_dataset = tf.data.Dataset.from_tensor_slices(num_list1)\n",
    "num_list2_dataset = tf.data.Dataset.from_tensor_slices(num_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator1 = tf.compat.v1.data.make_one_shot_iterator(num_list1_dataset)\n",
    "iterator2 = tf.compat.v1.data.make_one_shot_iterator(num_list2_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that running this cell a second time without restarting the kernel gives\n",
    "# an 'OutOfRangeError: End of sequence [Op:IteratorGetNextSync]' error\n",
    "# since we are using make_one_shot_iterator()\n",
    "for item in num_list1_dataset:\n",
    "    num = iterator1.get_next().numpy()\n",
    "    print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in num_list2_dataset:\n",
    "    num = iterator2.get_next().numpy()\n",
    "    print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy array in batches example, drop_remainder=False is the default\n",
    "num_list1_dataset = tf.data.Dataset.from_tensor_slices(num_list1).batch(3, drop_remainder = False)\n",
    "iterator = tf.compat.v1.data.make_one_shot_iterator(num_list1_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in num_list1_dataset:\n",
    "    num = iterator.get_next().numpy()\n",
    "    print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zipping datasets examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_list1_dataset = tf.data.Dataset.from_tensor_slices(num_list1)\n",
    "num_list2_dataset = tf.data.Dataset.from_tensor_slices(num_list2)\n",
    "zipped_datasets = tf.data.Dataset.zip((num_list1_dataset, num_list2_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = [1,2,3,4,5]\n",
    "dataset2 = ['a','e','i','o','u']\n",
    "dataset1 = tf.data.Dataset.from_tensor_slices(dataset1)\n",
    "dataset2 = tf.data.Dataset.from_tensor_slices(dataset2)\n",
    "zipped_datasets = tf.data.Dataset.zip((dataset1, dataset2))\n",
    "\n",
    "iterator = tf.compat.v1.data.make_one_shot_iterator(zipped_datasets)\n",
    "for item in zipped_datasets:\n",
    "    num = iterator.get_next()\n",
    "    print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate datasets example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = tf.data.Dataset.from_tensor_slices([1,2,3,5,7,11,13,17])\n",
    "ds2 = tf.data.Dataset.from_tensor_slices([19,23,29,31,37,41])\n",
    "ds3 = ds1.concatenate(ds2)\n",
    "print(ds3)\n",
    "iterator = tf.compat.v1.data.make_one_shot_iterator(ds3)\n",
    "for i in range(14):\n",
    "    num = iterator.get_next()\n",
    "    print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in fact, we don't even need an iterator\n",
    "# this for works just as well, and throws no OutOfRangeError\n",
    "# when used repeatedly\n",
    "epochs=2\n",
    "for e in range(epochs):\n",
    "    for item in ds3:\n",
    "        print(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use of  comma separated files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "filename = [\"./size_1000.csv\"]\n",
    "record_defaults = [tf.float32] * 2   # two required float columns\n",
    "dataset = tf.data.experimental.CsvDataset(filename, record_defaults, header=True, select_cols=[1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more examples of csv files, see files for structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"mycsvfile.txt\"\n",
    "record_defaults = [tf.float32, tf.constant([0.0], dtype=tf.float32), tf.int32,]\n",
    "dataset = tf.data.experimental.CsvDataset(filename, record_defaults, header=False, select_cols=[1,2,3])\n",
    "\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"file1.txt\"\n",
    "record_defaults = [tf.float32, tf.float32, tf.string ,]\n",
    "dataset = tf.data.experimental.CsvDataset(filename, record_defaults, header=False)\n",
    "for item in dataset:\n",
    "    print(item[0].numpy(), item[1].numpy(),item[2].numpy().decode() ) # decode as string is in binary format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another popular storage format is the TFRecord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "data=np.array([10.,11.,12.,13.,14.,15.])\n",
    "def npy_to_tfrecords(fname,data):\n",
    "    writer = tf.io.TFRecordWriter(fname)\n",
    "    feature={}\n",
    "\n",
    "    feature['data'] = tf.train.Feature(float_list=tf.train.FloatList(value=data))\n",
    "    example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    serialized = example.SerializeToString()\n",
    "    writer.write(serialized)\n",
    "    writer.close()\n",
    "npy_to_tfrecords(\"./myfile.tfrecords\",data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TFRecordDataset(\"./myfile.tfrecords\")\n",
    "\n",
    "def parse_function(example_proto):\n",
    "    keys_to_features = {'data':tf.io.FixedLenSequenceFeature([], dtype = tf.float32, allow_missing = True) }\n",
    "    parsed_features = tf.io.parse_single_example(serialized=example_proto, features=keys_to_features)\n",
    "    return parsed_features['data']\n",
    "\n",
    "dataset = dataset.map(parse_function)\n",
    "iterator = tf.compat.v1.data.make_one_shot_iterator(dataset)\n",
    "# array is retrieved as one item\n",
    "item = iterator.get_next()\n",
    "print(item)\n",
    "print(item.numpy())\n",
    "print(item[2].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create record\n",
    "\n",
    "filename = './students.tfrecords'\n",
    "data = {\n",
    "    'ID': 61553,\n",
    "    'Name': ['Jones', 'Felicity'],\n",
    "    'Scores': [45.6, 97.2] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = tf.train.Feature(int64_list=tf.train.Int64List(value=[data['ID']]))\n",
    "\n",
    "Name = tf.train.Feature(bytes_list=tf.train.BytesList(value=[n.encode('utf-8') for n in data['Name']]))\n",
    "\n",
    "Scores = tf.train.Feature(float_list=tf.train.FloatList(value=data['Scores']))\n",
    "\n",
    "example = tf.train.Example(features=tf.train.Features(feature={'ID': ID, 'Name': Name, 'Scores': Scores }))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.io.TFRecordWriter(filename)\n",
    "writer.write(example.SerializeToString())\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read record\n",
    "dataset = tf.data.TFRecordDataset(\"./students.tfrecords\")\n",
    "\n",
    "def parse_function(example_proto):\n",
    "    keys_to_features = {'ID':tf.io.FixedLenFeature([], dtype = tf.int64),\n",
    "                       'Name':tf.io.VarLenFeature(dtype = tf.string),\n",
    "                        'Scores':tf.io.VarLenFeature(dtype = tf.float32)\n",
    "                       }\n",
    "    parsed_features = tf.io.parse_single_example(serialized=example_proto, features=keys_to_features)\n",
    "    return parsed_features[\"ID\"], parsed_features[\"Name\"],parsed_features[\"Scores\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(parse_function)\n",
    "\n",
    "iterator = tf.compat.v1.data.make_one_shot_iterator(dataset)\n",
    "item = iterator.get_next()\n",
    "# record is retrieved as one item\n",
    "print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ID: \",item[0].numpy())\n",
    "name = item[1].values.numpy()\n",
    "name1= name[0].decode()\n",
    "name2 = name[1].decode()\n",
    "print(\"Name:\",name1,\",\",name2)\n",
    "print(\"Scores: \",item[2].values.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example uses the fashion-mnist dataset\n",
    "# Which is a dropin replacement for mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.datasets import fashion_mnist\n",
    "\n",
    "width, height, = 28,28\n",
    "n_classes = 10\n",
    "# load the dataset\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# normalise the features  for better training\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "\n",
    "# flatten the features  for use by the training algorithm\n",
    "x_train = x_train.reshape((60000, width * height))\n",
    "x_test = x_test.reshape((10000, width * height))\n",
    "\n",
    "split = 50000\n",
    "#split feature training  set into training and validation sets\n",
    "(x_train, x_valid) = x_train[:split], x_train[split:]\n",
    "(y_train, y_valid) = y_train[:split], y_train[split:]\n",
    "\n",
    "# one-hot encode the labels using TensorFLow.\n",
    "# then convert back to numpy as we cannot combine numpy\n",
    "# and tensors as input to keras later\n",
    "y_train_ohe = tf.one_hot(y_train, depth=n_classes).numpy()\n",
    "y_valid_ohe = tf.one_hot(y_valid, depth=n_classes).numpy()\n",
    "y_test_ohe = tf.one_hot(y_test, depth=n_classes).numpy()\n",
    "#or use tf.keras.utils.to_categorical(y_train,10), for example\n",
    "# show difference between original label and one-hot-encoded label\n",
    "i=5\n",
    "print(y_train[i]) # 'ordinary' number value of label at index i\n",
    "\n",
    "print(y_train_ohe[i]) # same value as a 1. in correct position in a length 10 1D numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-hot encoding is also useful where the labels are text categorical,\n",
    "# e.g. red, blue, green could be coded as [0,0,1], [0,1,0] and [1,0,0] for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatic differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by default, you can only call tape.gradient once in a GradientTape  context\n",
    "weight1 = tf.Variable(2.0)\n",
    "def weighted_sum(x1):\n",
    "    return weight1 * x1\n",
    "with tf.GradientTape() as tape:\n",
    "    sum = weighted_sum(7.)\n",
    "    [weight1_grad] = tape.gradient(sum, [weight1])\n",
    "print(weight1_grad.numpy()) # 7 , weight1*x diff w.r.t. weight1 is x, 7, also see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you need to call tape.gradient more than once\n",
    "# use GradientTape(persistent=True)\n",
    "weight1 = tf.Variable(2.0)\n",
    "weight2 = tf.Variable(3.0)\n",
    "weight3 = tf.Variable(5.0)\n",
    "\n",
    "def weighted_sum(x1, x2, x3):\n",
    "    return weight1*x1 + weight2*x2 +  weight3*x3\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    sum = weighted_sum(7.,5.,6.)\n",
    "[weight1_grad] = tape.gradient(sum, [weight1])\n",
    "[weight2_grad] = tape.gradient(sum, [weight2])\n",
    "[weight3_grad] = tape.gradient(sum, [weight3])\n",
    "\n",
    "print(weight1_grad.numpy()) # x1, 7\n",
    "print(weight2_grad.numpy()) # x2, 5\n",
    "print(weight3_grad.numpy()) # x3, 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
